### ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸° ###
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt


# ì–¸ë” ìƒ˜í”Œë§ #
from imblearn.under_sampling import RandomUnderSampler  # ëžœë¤ ì–¸ë”ìƒ˜í”Œë§
from imblearn.under_sampling import TomekLinks          # Tomek Links
from imblearn.under_sampling import CondensedNearestNeighbour    # CNN(Condensed Nearest Neighbors)
from imblearn.under_sampling import NearMiss            # NearMiss
from imblearn.under_sampling import ClusterCentroids    # ClusterCentroids


# ë°ì´í„° ì „ì²˜ë¦¬
from sklearn.preprocessing import MinMaxScaler # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
from sklearn.preprocessing import LabelEncoder # ë°ì´í„° ì¸ì½”ë”©
from zoneinfo import ZoneInfo # ì‹œê°„ ì¸ì½”ë”©

# ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import HistGradientBoostingClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier

# ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í‰ê°€ ì§€í‘œ
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import roc_curve, roc_auc_score
from sklearn.metrics import classification_report, confusion_matrix

# í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV



### ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ###
df_new = pd.read_csv(r'C:\Users\DATOP\data\dataset_002\HF_TRANS_TRAN_new.csv')
df_new.info()


## ë°ì´í„° ë³µì‚¬ ##
df = df_new.copy()


### ë°ì´í„° ê²°ì¸¡ì¹˜ í™•ì¸ ###
df.isna().sum()

df['ff_sp_ai'].value_counts(dropna=False)

## ë°ì´í„° ê²°ì¸¡ì¹˜ ì±„ìš°ê¸° ##
df['ff_sp_ai'].fillna(0, inplace=True)
df.isna().sum()

## ff_sp_ai ì»¬ëŸ¼ ë§µí•‘ - ì •ìƒê±°ëž˜ 0, ì˜ì‹¬ê±°ëž˜ 1
label_mapping = {0: 0, '01': 0, '02': 0, 1: 0, 2: 0, 'SP': 1}
df['ff_sp_ai'] = df['ff_sp_ai'].map(label_mapping)

df['ff_sp_ai'].value_counts()
df['ff_sp_ai'].dtype



### ë°ì´í„° íƒ€ìž… ë³€í™˜ ###
# íƒ€ìž… ë³€í™˜ ì „
df.info()

# tran_dt : int64 -> datetime
df['tran_dt'] = pd.to_datetime(df['tran_dt'], format='%Y%m%d')

# wd_fc_ac, dps_fc_ac, md_type, fnd_type : int64 -> category
df['wd_fc_ac'] = df['wd_fc_ac'].astype('category')
df['dps_fc_ac'] = df['dps_fc_ac'].astype('category')
df['md_type'] = df['md_type'].astype('category')
df['fnd_type'] = df['fnd_type'].astype('category')

# íƒ€ìž…ë³€í™˜ í›„ 
df.info()



### ë‚ ì§œ/ì‹œê°„ ì»¬ëŸ¼ ###
# ë‚ ì§œë°ì´í„° ë¶„ë¦¬
df['month'] = df['tran_dt'].dt.month
df['day'] = df['tran_dt'].dt.day
df['weekday'] = df['tran_dt'].dt.weekday  # 0: ì›”ìš”ì¼ ~ 6: ì¼ìš”ì¼

# ì£¼ê¸°ì„±ì„ ê°–ëŠ” sin,cos ì»¬ëŸ¼ ìƒì„±
df['hour_sin'] = round(np.sin(2 * np.pi * df['tran_tmrg'] / 24), 4)
df['hour_cos'] = round(np.cos(2 * np.pi * df['tran_tmrg'] / 24), 4)
df['weekday_sin'] = round(np.sin(2 * np.pi * df['weekday'] / 7), 4)
df['weekday_cos'] = round(np.cos(2 * np.pi * df['weekday'] / 7), 4)
df.head()



### í›ˆë ¨, ê²€ì¦, ì‹œí—˜ ë°ì´í„°ë¡œ ë‚˜ëˆ„ê¸° ###
# ì»¬ëŸ¼ ë¶„ë¦¬
features = ['wd_fc_ac', 'dps_fc_ac', 'md_type', 'fnd_type',
            'tran_amt', 'month', 'day', 'hour_sin', 'hour_cos',
            'weekday_sin', 'weekday_cos']
target = 'ff_sp_ai'


## ë°ì´í„° ë‚˜ëˆ„ê¸°
X_train = df.loc[(df['month'] >= 1) & (df['month'] < 10), features]  # 1ì›” ~ 9ì›”
X_val = df.loc[df['month'] == 10, features]                          # 10ì›” 
X_test = df.loc[(df['month'] >= 11) & (df['month'] <= 12), features]  # 11~12ì›”

y_train = df.loc[(df['month'] >= 1) & (df['month'] < 10), target]    # 1ì›” ~ 9ì›”
y_val = df.loc[df['month'] == 10, target]                            # 10ì›”
y_test = df.loc[(df['month'] >= 11) & (df['month'] <= 12), target]   # 11~12ì›”

print("í›ˆë ¨ë°ì´í„°:", X_train.shape, y_train.shape)
print("ê²€ì¦ë°ì´í„°:", X_val.shape, y_val.shape)
print("ì‹œí—˜ë°ì´í„°:", X_test.shape, y_test.shape)


## í›ˆë ¨ë°ì´í„° í™•ì¸
display(X_train.head(3))
display(X_train.tail(3))
y_train.value_counts(normalize=True)


### ì–¸ë”ìƒ˜í”Œë§ - RandomUnderSampling ###
# ë¹„ìœ¨ ì„¤ì • (0.99,...,0.90)
ratios = np.arange(0.99, 0.89, -0.01)

# ìƒ˜í”Œë§ ê²°ê³¼ë¥¼ ì €ìž¥í•  ë”•ì…”ë„ˆë¦¬
sampled_datasets = {}

# ì˜ì‹¬ê±°ëž˜ê°œìˆ˜
num_minority = np.sum(y_train == 1)

for ratio in ratios:
    # ì •ìƒê±°ëž˜ì˜ ëª©í‘œ ê°œìˆ˜ ì„¤ì •
    num_majority = int(num_minority / (1 - ratio) - num_minority)

    # RandomUnderSampler ì ìš©
    rus = RandomUnderSampler(sampling_strategy={0: num_majority, 1: num_minority}, random_state=42)
    X_resampled, y_resampled = rus.fit_resample(X_train, y_train)

    # ì €ìž¥
    sampled_datasets[1-ratio] = (X_resampled, y_resampled)

    # ë°ì´í„° ê°œìˆ˜ í™•ì¸
    print(f"ì˜ì‹¬ê±°ëž˜ë¹„ìœ¨ {ratio:.2f}, ì •ìƒê±°ëž˜ê±´ìˆ˜: {num_majority}, ì‚¬ê¸°ê±°ëž˜ê±´ìˆ˜: {num_minority}")



### ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë§ ###
## LightGBM
# í‰ê°€ ì§€í‘œë¥¼ ì €ìž¥í•  ë¦¬ìŠ¤íŠ¸
results = []
confusion_matrices=[]

# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • (ê¸°ë³¸ê°’, í•„ìš”ì‹œ ë³€ê²½ ê°€ëŠ¥)
lgb_params = {
    'objective': 'binary',
    'boosting_type': 'gbdt',
    'num_leaves': 5,
    'learning_rate': 0.05,
    'n_estimators': 500,
    'verbose': -1,
    'max_depth':7,
    'random_state':42
}

# ê° ë¹„ìœ¨ë³„ë¡œ LightGBM ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
for ratio, (X_train_resampled, y_train_resampled) in sampled_datasets.items():
    print(f"\nðŸ”¹ ì˜ì‹¬ê±°ëž˜ë¹„ìœ¨ {ratio:.2f}ë¡œ LightGBM í•™ìŠµ ì¤‘...")

    # LightGBM ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
    model = LGBMClassifier(**lgb_params)
    model.fit(X_train_resampled, y_train_resampled, eval_set=[(X_val, y_val)])

    # ì˜ˆì¸¡ ìˆ˜í–‰
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]  

    # í‰ê°€ ì§€í‘œ ê³„ì‚°
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred, zero_division=0)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_pred_proba)

    # í˜¼ë™ í–‰ë ¬ ê³„ì‚°
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()

    # FPR ê³„ì‚°
    fpr = fp / (fp + tn)
            
    # ê²°ê³¼ ì €ìž¥
    results.append({
        "sp_ratio": ratio, "Accuracy": acc, "Precision": prec, "Recall": rec, 
        "F1-score": f1, "ROC-AUC": roc_auc, "FPR": fpr
    })

    confusion_matrices.append(confusion_matrix(y_test, y_pred))

# ê²°ê³¼ ì •ë¦¬
df_results = pd.DataFrame(results)

# ì„±ëŠ¥ ë¹„êµ ì¶œë ¥
print("\nðŸ“Š LightGBM ì„±ëŠ¥ ë¹„êµ ê²°ê³¼")
print(df_results)

## ì„±ëŠ¥ ì§€í‘œ ì‹œê°í™” ##

# ê·¸ëž˜í”„ í¬ê¸° ì„¤ì •
plt.figure(figsize=(10, 6))

# ì„±ëŠ¥ ì§€í‘œ ê·¸ëž˜í”„
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score', 'ROC-AUC', 'FPR']

# ê° ì„±ëŠ¥ ì§€í‘œë¥¼ ê·¸ëž˜í”„ë¡œ í‘œì‹œ
for metric in metrics:
   plt.plot(df_results['sp_ratio'],
            df_results[metric],
            marker='o',
            label=metric)

# ê·¸ëž˜í”„ ì œëª© ë° ë¼ë²¨ ì„¤ì •
plt.title("Performance Metrics by Random UnderSampling Ratio(LGBM)", fontsize=14)
plt.xlabel("Sampling Ratio (Proportion of Minority Class)", fontsize=12)
plt.ylabel("Metric Score", fontsize=12)
plt.xticks(df_results['sp_ratio'])
plt.legend(title="Metrics")
plt.grid(True)

# ê·¸ëž˜í”„ ì¶œë ¥
plt.show()

# confusion_matrix ì‹œê°í™”(ê°€ìž¥ ë‚˜ì€ ì„±ëŠ¥ìœ¼ë¡œ)
best_lgbm_cm = confusion_matrices[0]
group_names = ["TN", "FP", "FN", "TP"]
group_counts = [value for value in best_lgbm_cm.flatten()]
group_percentages = [f"{value:.4%}" for value in best_lgbm_cm.flatten()/np.sum(best_lgbm_cm)]
labels = [f"{name}\n{count}\n({percent})" for name, count, percent in zip(group_names, group_counts, group_percentages)]
labels = np.asarray(labels).reshape(2, 2)

sns.heatmap(best_lgbm_cm, annot=labels, fmt='', cmap='Blues')
plt.title('Confusion Matrix(LightGBM, sp_ratio 0.01)')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()


## Catboost
# í‰ê°€ ì§€í‘œë¥¼ ì €ìž¥í•  ë¦¬ìŠ¤íŠ¸
results = []
confusion_matrices=[]

# CatBoostClassifier í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
cat_params = {
    'iterations': 500,
    'learning_rate': 0.05,
    'depth': 6,
    'verbose': 100,
    'early_stopping_rounds': 50,
    'cat_features':['wd_fc_ac', 'dps_fc_ac','md_type', 'fnd_type']
}

# ê° ë¹„ìœ¨ë³„ë¡œ CatBoost ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
for ratio, (X_train_resampled, y_train_resampled) in sampled_datasets.items():
    print(f"\nðŸ”¹ ì˜ì‹¬ê±°ëž˜ë¹„ìœ¨ {ratio:.2f}ë¡œ CatBoostClassifier í•™ìŠµ ì¤‘...")

    # CatBoost ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
    model = CatBoostClassifier(**cat_params)
    model.fit(
        X_train_resampled, y_train_resampled,
        eval_set=(X_val, y_val),
        use_best_model=True
    )

    # ì˜ˆì¸¡ ìˆ˜í–‰
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]  

    # í‰ê°€ ì§€í‘œ ê³„ì‚°
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred, zero_division=0)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_pred_proba)

    # í˜¼ë™ í–‰ë ¬ ê³„ì‚°
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()

    # FPR ê³„ì‚°
    fpr = fp / (fp + tn)

    # ê²°ê³¼ ì €ìž¥
    results.append({
        "sp_ratio": ratio, "Accuracy": acc, "Precision": prec, "Recall": rec, 
        "F1-score": f1, "ROC-AUC": roc_auc, "FPR": fpr
    })

    confusion_matrices.append(confusion_matrix(y_test, y_pred))

# ê²°ê³¼ ì •ë¦¬
df_results = pd.DataFrame(results)

# ì„±ëŠ¥ ë¹„êµ ì¶œë ¥
print("\nðŸ“Š CatBoost ì„±ëŠ¥ ë¹„êµ ê²°ê³¼")
print(df_results)

## ì„±ëŠ¥ ì§€í‘œ ì‹œê°í™” ##

# ê·¸ëž˜í”„ í¬ê¸° ì„¤ì •
plt.figure(figsize=(10, 6))

# ì„±ëŠ¥ ì§€í‘œ ê·¸ëž˜í”„
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score', 'ROC-AUC', 'FPR']

# ê° ì„±ëŠ¥ ì§€í‘œë¥¼ ê·¸ëž˜í”„ë¡œ í‘œì‹œ
for metric in metrics:
   plt.plot(df_results['sp_ratio'],
            df_results[metric],
            marker='o',
            label=metric)

# ê·¸ëž˜í”„ ì œëª© ë° ë¼ë²¨ ì„¤ì •
plt.title("Performance Metrics by Random UnderSampling Ratio(CatBoost)", fontsize=14)
plt.xlabel("Sampling Ratio (Proportion of Minority Class)", fontsize=12)
plt.ylabel("Metric Score", fontsize=12)
plt.xticks(df_results['sp_ratio'])
plt.legend(title="Metrics")
plt.grid(True)

# ê·¸ëž˜í”„ ì¶œë ¥
plt.show()

# confusion_matrix ì‹œê°í™”(ê°€ìž¥ ë‚˜ì€ ì„±ëŠ¥ìœ¼ë¡œ)
best_cbt_cm = confusion_matrices[0]
group_names = ["TN", "FP", "FN", "TP"]
group_counts = [value for value in best_cbt_cm.flatten()]
group_percentages = [f"{value:.4%}" for value in best_cbt_cm.flatten()/np.sum(best_cbt_cm)]
labels = [f"{name}\n{count}\n({percent})" for name, count, percent in zip(group_names, group_counts, group_percentages)]
labels = np.asarray(labels).reshape(2, 2)

sns.heatmap(best_cbt_cm, annot=labels, fmt='', cmap='Blues')
plt.title('Confusion Matrix(CatBoost, sp_ratio 0.01)')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()


## XGBoost
# í‰ê°€ ì§€í‘œë¥¼ ì €ìž¥í•  ë¦¬ìŠ¤íŠ¸
results = []
confusion_matrices=[]

# XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • (ê¸°ë³¸ê°’, í•„ìš”ì‹œ ë³€ê²½ ê°€ëŠ¥)
xgb_params = {
    'objective': 'binary:logistic',
    'eval_metric': 'logloss',
    'learning_rate': 0.05,
    'max_depth': 7,
    'n_estimators': 500,
    'early_stopping_rounds': 50,
    'enable_categorical':True,
    'seed':42
}

# ê° ë¹„ìœ¨ë³„ë¡œ XGBoost ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
for ratio, (X_train_resampled, y_train_resampled) in sampled_datasets.items():
    print(f"\nðŸ”¹ ì˜ì‹¬ê±°ëž˜ë¹„ìœ¨ {ratio:.2f}ë¡œ XGBoostClassifier í•™ìŠµ ì¤‘...")

    # XGBoost ëª¨ë¸ ìƒì„±
    model = xgb.XGBClassifier(**xgb_params)

    # í•™ìŠµ ì§„í–‰ (ê²€ì¦ ë°ì´í„° í¬í•¨)
    model.fit(
        X_train_resampled, y_train_resampled,
        eval_set=[(X_val, y_val)],
        verbose = 100
    )

    # ì˜ˆì¸¡ ìˆ˜í–‰
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]  

    # í‰ê°€ ì§€í‘œ ê³„ì‚°
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred, zero_division=0)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_pred_proba)

    # í˜¼ë™ í–‰ë ¬ ê³„ì‚°
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()

    # FPR ê³„ì‚°
    fpr = fp / (fp + tn)

    # ê²°ê³¼ ì €ìž¥
    results.append({
        "sp_ratio": ratio, "Accuracy": acc, "Precision": prec, "Recall": rec, 
        "F1-score": f1, "ROC-AUC": roc_auc, "FPR": fpr
    })

   confusion_matrices.append(confusion_matrix(y_test, y_pred))

# ê²°ê³¼ ì •ë¦¬
df_results = pd.DataFrame(results)

# ì„±ëŠ¥ ë¹„êµ ì¶œë ¥
print("\nðŸ“Š XGBoost ì„±ëŠ¥ ë¹„êµ ê²°ê³¼")
print(df_results)

## ì„±ëŠ¥ ì§€í‘œ ì‹œê°í™” ##

# ê·¸ëž˜í”„ í¬ê¸° ì„¤ì •
plt.figure(figsize=(10, 6))

# ì„±ëŠ¥ ì§€í‘œ ê·¸ëž˜í”„
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score', 'ROC-AUC', 'FPR']

# ê° ì„±ëŠ¥ ì§€í‘œë¥¼ ê·¸ëž˜í”„ë¡œ í‘œì‹œ
for metric in metrics:
   plt.plot(df_results['sp_ratio'],
            df_results[metric],
            marker='o',
            label=metric)

# ê·¸ëž˜í”„ ì œëª© ë° ë¼ë²¨ ì„¤ì •
plt.title("Performance Metrics by Random UnderSampling Ratio(XGBoost)", fontsize=14)
plt.xlabel("Sampling Ratio (Proportion of Minority Class)", fontsize=12)
plt.ylabel("Metric Score", fontsize=12)
plt.xticks(df_results['sp_ratio'])
plt.legend(title="Metrics")
plt.grid(True)

# ê·¸ëž˜í”„ ì¶œë ¥
plt.show()

# confusion_matrix ì‹œê°í™”(ê°€ìž¥ ë‚˜ì€ ì„±ëŠ¥ìœ¼ë¡œ)
best_xgb_cm = confusion_matrices[0]
group_names = ["TN", "FP", "FN", "TP"]
group_counts = [value for value in best_xgb_cm.flatten()]
group_percentages = [f"{value:.4%}" for value in best_xgb_cm.flatten()/np.sum(best_xgb_cm)]
labels = [f"{name}\n{count}\n({percent})" for name, count, percent in zip(group_names, group_counts, group_percentages)]
labels = np.asarray(labels).reshape(2, 2)

sns.heatmap(best_xgb_cm, annot=labels, fmt='', cmap='Blues')
plt.title('Confusion Matrix(XGBoost, sp_ratio 0.01)')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()
